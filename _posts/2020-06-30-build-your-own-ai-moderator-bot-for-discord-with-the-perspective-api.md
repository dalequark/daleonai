---
layout: post
title: Build Your Own AI Moderator Bot for Discord with the Perspective API
description: Build a moderator bot that instantly flags toxic, insulting, flirtation,
  or spam messages with machine learning--no data science experience required.
date: 2020-06-30 05:00:00 +0000
feature_image: "/images/copy-of-flag-toxic-flirty-or-spam-messages-instantly-with-an-ai-moderator-bot-for-discord.png"
tags:
- google cloud
- natural language processing
- machine learning
- chatbots

---
_In this post, I'll show you how to build an AI-powered moderator bot for the Discord chat platform using the Perspective API._

<!--more-->

<iframe width="560" height="315" src="https://www.youtube.com/embed/ABr_HkO0eGk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Iâ€™ve been fascinated by the topic of moderation--deciding who gets to post what on the internet--ever since I started working at the online dating site OkCupid, five years ago . The moderation team there was responsible for the [near-impossible task](https://www.thecut.com/2017/02/banned-from-okcupid-sexting-moderation.html) of drawing the line between which messages counted as riskÃ© flirtation (usually ok), illicit come-ons (possibly ok), and sexual harassment (which would get you banned). As RadioLab put it in their [excellent podcast episode](https://www.wnycstudios.org/podcasts/radiolab/articles/post-no-evil) on the topic, â€œHow much butt is too much butt?â€ Questions like these are tough enough, and then, if youâ€™re Twitter, you have to decide what to do when the Presidentâ€™s tweets [violate your Terms of Service](https://www.nytimes.com/2020/06/05/technology/twitter-trump-facebook-moderation.html).

Itâ€™s a dirty job, but someoneâ€™s got to do it. Or do they? Can an AI handle moderation instead?

Some policy questions, like what to do with the Presidentâ€™s tweets or how to define hate speech, have no right answer. But in many more instances, and for many more platforms, bad content is easy to spot. You probably canâ€™t share any sort of nudity or gore or hate speech on a professional networking app or an educational site for children. Plus, since most apps arenâ€™t public forums like Facebook or Twitter (where we have strong expectations of free speech), the consequences of being too harsh or conservative in filtering risky content are lower. For these applications, machine learning can really help.

In this post, Iâ€™ll show you how to build your own AI-powered moderation bot for the chat platform Discord. Donâ€™t worry if youâ€™ve never done any machine learning before--weâ€™ll use the [Perspective API](https://www.perspectiveapi.com/), a completely free tool from Google, to handle the complicated bits.

But, before we get into the tech-y details, letâ€™s talk about some high-level moderation strategies. Most companies use one of three approaches:

1. **Pre-Moderation** is when a team of human moderators review every single piece of content before itâ€™s ever posted. Itâ€™s a good approach when itâ€™s _very, very_ important that no â€œbadâ€ content slips through the cracks. Apple, for example, requires every app submitted to the App Store to be[ reviewed by an employee](https://www.cnbc.com/2019/06/21/how-apples-app-review-process-for-the-app-store-works.html) before it's published.
2. **Post-Moderation** is the opposite--content is allowed to be posted _before_ itâ€™s reviewed by moderators. Instead, the job of flagging posts usually gets crowdsourced to users, who are able to â€œflagâ€ or â€œreportâ€ content they believe violates a siteâ€™s TOS. You see this almost everywhere (YouTube, Facebook, Instagram, and many more).

![Screenshot of reporting a video on YouTube](/images/screen-shot-2020-06-30-at-12-42-05-pm.png "Reporting a video on youtube")

Both of these approaches clearly come with drawbacks--pre-moderation requires a large human moderation team, and doesnâ€™t work for real-time applications (chat, or any type of streaming). Post-moderation scales better, but forces your users to consume potentially offensive or disturbing media. Which leads us to:

1. **AI-Powered Moderation**. This approach uses AI to flag inappropriate content the moment itâ€™s created and prevent it from ever surfacing. For example, the online gaming platform FACEIT [uses this technique](https://www.forbes.com/sites/mikestubbs/2019/10/23/faceit-and-google-partner-to-use-ai-to-tackle-in-game-toxicity/#1d78ab2079d0) to detect offensive messages in chat, send users warnings, and even potentially kick them from play.

AI moderators work around the clock and react near-instantaneously, so they can spot, for example, a livestream of a gory video seconds after it starts. Usually, AIs donâ€™t _replace_ a human moderation team, but work in conjunction with one, helping to automatically prioritize content that needs to be checked.

So, without further ado, letâ€™s build an AI-powered moderator bot for the chat platform Discord.

## Building an AI-Powered Moderation Bot for Discord

_Want to jump straight to the code? Check it out_ [_here_](https://github.com/google/making_with_ml/tree/master/discord_moderator)_._

Today Iâ€™ll show you how to build an AI-powered moderation bot for Discord.

The bot sits in a Discord channel and analyzes all messages users to see if theyâ€™re toxic, nonsensical, flirty, insulting, or spam-y. When it detects that a message does fit one of these buckets (i.e. toxic) it â€œreactsâ€ to the message with an emoji (ðŸ§¨). Later, you can check to see how many reactions each user has received using the â€œ!karmaâ€ hot word. And if a user sends too many toxic messages, they get kicked from the channel.

![](/images/discord_moderator.gif)

Letâ€™s see how to build it.

### Setup

First letâ€™s grab the code for our moderator bot. Clone the \`making_with_ml\` [Github repo](https://github.com/google/making_with_ml) and navigate to the \`discord_moderator\` folder:

    > git clone git@github.com:google/making_with_ml.git
    > cd making_with_ml/discord_moderator

Hereâ€™s all the code youâ€™ll need to run your ML moderator bot. Before we can run the bot, weâ€™ll need to get some (completely free) services set up. First, make a copy of the file `.env_template` and name it `.env`.

    > cp .env_template .env`

Open that file up in your favorite text editor:

    PERSPECTIVE_API_KEY="YOUR_API_KEY"
    DISCORD_TOKEN="YOUR_DISCORD_TOKEN"
    KICK_THRESHOLD=4

As you can see, weâ€™ll need a couple of different API and developer tokens to get started, one for the Perspective API, which weâ€™ll use for analyzing messages, and one for Discord (more on that in a second).

What is the Perspective API? The Perspective API is a free tool built by Jigsaw, a unit within Google that, in their own words, â€œforecasts and confronts emerging threats, creating future-defining research and technology to keep our world safer.â€ The Perspective API is one such tool they provide for keeping the (digital) world safer. It takes text as input (â€œYou stink like buttsâ€) and analyzes it for attributes like toxicity, insults, profanity, identity attacks, sexual explicitness, flirtation, threats, and more. You can quickly try it out in their interactive web demo:

![Example of Perspective interactive demo](/images/screen-shot-2020-06-30-at-11-10-09-am.png "Example of Perspective interactive demo")

Today, The New York Times uses Perspective to help automatically moderate their comments section.

Unfortunately, the documentation to actually use the Perspective API in Javascript is a bit sparse, so Iâ€™ll fill in some of the details here.

### Enabling Perspective

First, sign in to your Google Cloud account (itâ€™s [free to get started](https://console.cloud.google.com)), and either create a new project or select an existing one. In your project, enable the [Perspective Comment Analyzer API](https://console.cloud.google.com/apis/api/commentanalyzer.googleapis.com/overview). Youâ€™ll have to fill out a short survey to gain access (you should receive an email in a couple of hours).

Next youâ€™ll need to generate an API key to access the API in code. In the Google Cloud console left hand menu, click API & Services -> Credentials. On that screen, click "+ Create Credentials" -> "API key". Copy that API key.

![Creating an api key from the google developer console](/images/screen-shot-2020-06-30-at-12-44-05-pm.png "Create an API key")

Now go back to that file you created earlier--`.env`--and drop the key into the `PERSPECTIVE_API_KEY` field:

        PERSPECTIVE_API_KEY="YOUR_API_KEY"  \\paste yer key here
        DISCORD_TOKEN="YOUR_DISCORD_TOKEN"
        KICK_THRESHOLD=4

\##Analyzing Messages

Now you should be able to use the Perspective API to analyze text in code. To see an example, check out the file `perspective.js`. At the top of the file, youâ€™ll see all of the possible attributes the API can recognize:

    // Some supported attributes
    // attributes = ["TOXICITY", "SEVERE_TOXICITY", "IDENTITY_ATTACK", "INSULT",
    // "PROFANITY", "THREAT", "SEXUALLY_EXPLICIT", "FLIRTATION", "SPAM",
    // "ATTACK_ON_AUTHOR", "ATTACK_ON_COMMENTER", "INCOHERENT",
    // "INFLAMMATORY", "OBSCENE", "SPAM", "UNSUBSTANTIAL"];

On the next line, youâ€™ll see the attributes weâ€™ll actually using in our bot:

    // Set your own thresholds for when to trigger a response
    const attributeThresholds = {
     'INSULT': 0.75,
     'TOXICITY': 0.75,
     'SPAM': 0.75,
     'INCOHERENT': 0.75,
     'FLIRTATION': 0.75,
    };

See all those numbers next to each attribute? When you ask the Perspective API to analyze a comment (â€œYouâ€™re soooo sexyâ€), it returns a â€œsummaryScoreâ€ for each attribute:

    â€œattributeScoresâ€: {
    	â€œFLIRTATIONâ€: {
    		â€œsummaryScoreâ€: {
    			â€œValueâ€: 0.88309
    		}
    	}
    }

The score represents roughly how confident the machine learning model is that a comment is really flirtation or toxic or threatening, etc. The job is then on you, the developer, to choose a â€œcutoffâ€ for deciding when a comment should really get a label. Thatâ€™s what all those numbers mean in the `attributeThreshold` object I posted above. Iâ€™ll only consider a comment insulting or toxic or threatening if the summaryScore is above 0.75.

Pro tip: In your own application, youâ€™ll want to choose a cutoff that aligns with how your human moderation team (if you have one) is already moderating content. For example, on Tinder, sending flirtatious messages is totally ok, and we might have a higher cutoff for filtering sexually explicit messages than, say, a site like LinkedIn.

Meanwhile, take a look at the function `analyzeText` to see how we actually call the Perspective API:

    /**
    * Analyze attributes in a block of text
    * @param {string} text - text to analyze
    * @return {json} res - analyzed atttributes
    */
    async function analyzeText(text) {
     const analyzer = new googleapis.commentanalyzer_v1alpha1.Commentanalyzer();
    
     // This is the format the API expects
     const requestedAttributes = {};
     for (const key in attributeThresholds) {
       requestedAttributes[key] = {};
     }
    
     const req = {
       comment: {text: text},
       languages: ['en'],
       requestedAttributes: requestedAttributes,
     };
    
     const res = await analyzer.comments.analyze({
       key: process.env.PERSPECTIVE_API_KEY,
       resource: req},
     );
    
     data = {};
    
     for (const key in res['data']['attributeScores']) {
       data[key] =
           res['data']['attributeScores'][key]['summaryScore']['value'] >
           attributeThresholds[key];
     }
     return data;
    }

To actually connect with the API, we call `const analyzer = new googleapis.commentanalyzer_v1alpha1.Commentanalyzer();`. We then package up a request on line 21, specifying our language and the attributes we want to analyze, and send it to the API. Thatâ€™s it! On line 30, we check to see if the scores returned from the Perspective API are above our threshold (0.75).

Congratulations, you can now use machine learning to analyze text! Now letâ€™s throw that useful functionality into a Discord bot.

### Setting Up a Discord Bot

If youâ€™ve never used [Discord](https://discord.com/), itâ€™s a voice, video, and text chat platform thatâ€™s popular with gamers. You can use the methods in this post to build a bot for other messaging platforms, like [Hangouts](https://discord.com/) or [Slack](https://slack.com/), but I chose Discord because itâ€™s got such a delightful developer experience.

To get started, download Discord (or use the web version), and [sign up](https://discord.com/developers) for a Discord developer account. Once youâ€™re in, click â€œNew Application,â€ and give your new app a name and a description.

![Create a new Discord application](/images/screen-shot-2020-06-30-at-12-44-15-pm.png "Create a new Discord application")

On the left hand panel, choose â€œBotâ€ to create a new bot. Select "Add Bot." Give your new Bot a username and upload a cute or intimidating user icon.

![](/images/screen-shot-2020-06-30-at-10-41-42-am.png)

To be able to control your bot in code, youâ€™ll need a Discord developer token, which you can grab straight from that bot page by clicking â€œCopy.â€ Drop that code in your \`.env\` file:

    PERSPECTIVE_API_KEY="YOUR_API_KEY"
    DISCORD_TOKEN="YOUR_DISCORD_TOKEN" \\ your Discord token here
    KICK_THRESHOLD=4

Now that youâ€™ve created a bot in Discord, you can immediately add it to a channel. In any Discord app (Iâ€™m using the desktop app), log in and create a new server:

![Create a new Discord server](/images/screen-shot-2020-06-30-at-11-46-53-am.png "Create a new Discord server")

Now letâ€™s add your bot to the server. Back in the Discord [Developer Portal](https://discord.com/developers), in your application, click on OAuth on the left side panel

Discord has a very nice system for handling bot permissions. Under â€œSCOPES,â€ tick off the box next to â€œbot.â€ This should open a â€œBOT PERMISSIONSâ€ section below.

![Setting bot permissions on Discord](/images/screen-shot-2020-06-30-at-11-51-40-am.png "Setting bot permissions on Discord")

Tick the permissions â€œSend Messages,â€ â€œAdd Reactionsâ€ (for reacting to message with emojis), and â€œKick Membersâ€ (to bad ban members from the channel).

If you scroll up, just above the BOT PERMISSIONS panel, you should see a url, like: `https://discord.com/api/oauth2/authorize?client_id=YOUR_CLIENT_ID&permissions=2114&scope=bot`. Paste that url in your browser. If everythingâ€™s set up correctly, you should be able to add your bot to your server. And there you go! You created your first Discord bot without writing a single line of code. But right now, it doesnâ€™t do anything. You need to give it a brain.

### Building a Discord Moderator

In this project, our botâ€™s brain lives in the file `discord.js.`

At the top of the file, we import the Perspective file I talked about earlier:

`const perspective = require('./perspective.js');`

Then we set up an emoji map, which tells the bot how to react when various attributes are detected:

    // Set your emoji "awards" here
    const emojiMap = {
     'FLIRTATION': 'ðŸ’‹',
     'TOXICITY': 'ðŸ§¨',
     'INSULT': 'ðŸ‘Š',
     'INCOHERENT': 'ðŸ¤ª',
     'SPAM': 'ðŸŸ',
    };

Feel free to change those to whatever youâ€™d like.

What weâ€™d like our bot to do is analyze every message in a channel. We do that by creating a new Discord client and passing our developer token (thatâ€™s the last line in the file):

    // Log our bot in using the token from https://discordapp.com/developers/applications/me
    const client = new Discord.Client();
    client.login(process.env.DISCORD_TOKEN);

To listen for messages, we write a function that listens on the client `message` event:

    client.on('message', async (message) => {
     // Ignore messages that aren't from a guild
     // or are from a bot
     if (!message.guild || message.author.bot) return;
    
     // If we've never seen a user before, add them to memory
     const userid = message.author.id;
     if (!users[userid]) {
       users[userid] = [];
     }
    
     // Evaluate attributes of user's message
     let shouldKick = false;
     try {
       shouldKick = await evaluateMessage(message);
     } catch (err) {
       console.log(err);
     }
     if (shouldKick) {
       kickBaddie(message.author, message.guild);
       delete users[message.author.id];
       message.channel.send(`Kicked user ${message.author.username} from channel`);
       return;
     }
    
    
     if (message.content.startsWith('!karma')) {
       const karma = getKarma(message);
       message.channel.send(karma ? karma : 'No karma yet!');
     }
    });

Thereâ€™s a lot going on here.

* On line 4, we make sure that the messages we're analyzing only come from other users (not bots)
* On 8, we allocate some memory to keep track of our users. This is how we'll remember how many emojis we've given them, and how many times they've said toxic things.
* On 14, we call the `evaluateMessage` function, which uses the Perspective API to analyze a user's message. That function (which you can further investigate in the file) passes text to the Perspective API, responds with an emoji reaction if an attribute is found, and counts up the number of times a user has said something toxic. If it's more than `KICK_THRESHOLD`, a value set in our `.env` file, the function returns True (i.e. we should kick the user from the channel).
* On line 19, we actually kick the user from the channel, using the function `kickBaddie`.
* Finally, on line 27, we watch for the "!karma" hot word. If a user types this hot word, we'll send a message with a roundup of the stats for users in the channel.

To really see what's going on here in detail, you'll have to look at the functions `evaluateMessage` and `kickBaddie` in the file. I've added lots of documentation in line. But in a nutshell, that's all there is to it.

So there you go--you have your own AI-powered moderator bot for Discord. What do you think? And what do you want to learn how to build next? Let me know in the comments below, but guess what? Disqus uses the Perspective API to flag toxic comments too. ðŸ˜‰