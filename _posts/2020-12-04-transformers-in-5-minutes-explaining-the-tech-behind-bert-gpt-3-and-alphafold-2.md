---
layout: post
title: "Transformers in 5 Minutes: Explaining the Tech Behind BERT, GPT-3, and
  AlphaFold 2"
date: 2020-12-04T17:38:46.694Z
description: Transformers and attention are the new gold standard in modern NLP.
  Here's how they work
feature_image: /images/about.jpg
thumbnail_image: /images/about.jpg
tags:
  - nlp
permalink: how-transformers-bert-gpt3-attention-works
---
You know that expression *When you have a hammer, everything looks like a nail*? Well, in machine learning, it seems like we really have discovered a magical hammer for which everything is, in fact, a nail. It's called the Transformer, and it's the neural network architecture that underlies tons of recent breakthroughs in machine learning, like BERT (the NLP model that [underlies Google Search](https://blog.google/products/search/search-language-understanding-bert/)), [GPT-3](https://daleonai.com/gpt3-explained-fast) (the scary-good poem/essay/op-ed/code/html/SQL writing bot made by OpenAI), and, now [AlphaFold 2](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) (the model that cracked the protein-folding problem). It's fair to say that today, if you want to work in machine learning, you have to at least understand how to use Transformers, and probably also understand how they work.

In this post, I'll try to give you an intuition for how Transformers work in under five minutes. *Cracks knuckles*.