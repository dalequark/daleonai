---
layout: post
title: "Machine Learning's Most Useful Multitool: Embeddings"
date: 2022-03-01T16:30:55.384Z
description: Are embeddings machine learning's most underrated but super useful tool?
feature_image: /images/apple-watch.jpg
thumbnail_image: /images/apple-watch.jpg
tags:
  - ai
  - nlp
  - computervision
  - ml
permalink: embeddings-explained
---
Embeddings are one of the most versatile techniques in machine learning, and a critical tool every ML engineer should have in their tool belt. Itâ€™s a shame, then, that so few of us understand what they are and what theyâ€™re good for!

The problem, maybe, is that embeddings sound slightly abstract and esoteric:

*In machine learning, an embedding is a way of representing data as points in n-dimensional space so that similar data points cluster together.*

Sound boring and unimpressive? Donâ€™t be fooled. Because once you understand this ML multitool, youâ€™ll be able to build everything from search engines to recommendation systems to chatbots, and a whole lot more. Plus, you donâ€™t have to be a data scientist with ML expertise to use them, nor do you need a huge labeled dataset.

Have I convinced you how neat these bad boys are? ğŸ¤

Good. Letâ€™s dive in. In this post, weâ€™ll explore:

* What embeddings are
* What theyâ€™re used for
* Where and how to find open-source embedding models
* How to use them
* How to build your own embeddings

## What can you build with embeddings?

Before we talk about what embeddings are, letâ€™s take quick stock of what you can build with them. (You know--to whet your appetite.)

1. Recommendation systems (i.e. Netflix-style if-you-like-these-movies-youâ€™ll-like-this-one-too)
2. All kinds of search

   * Text search (like Google Search)
   * Image search (like Google search-by-image)
   * Music search (Ã  la "what song is this?")
3. Chatbots and question-answering systems
4. Data preprocessing (preparing data to be fed into a machine learning model)
5. One-shot/zero-shot learning (i.e. machine learning models that learn from almost no training data)
6. Fraud detection/outlier detection
7. Typo detection and all manners of â€œfuzzy matchingâ€
8. Detecting when ML models go stale (drift)
9. So much more!

Even if youâ€™re not trying to do something on this list, the applications of embeddings are so broad that you should probably keep reading, just in case. Right?

## What are Embeddings?

Embeddings are a way of representing dataâ€“almost any kind of data, like text, images, videos, users, music, whateverâ€“as points in space where the locations of those points in space are semantically meaningful.

The best way to intuitively understand what this means is by example, so letâ€™s take a look at one of the most famous embeddings, Word2Vec.

Word2Vec (short for word to vector) was a technique invented by Google in 2013 for embedding words. It takes as input a word and spits out an n-dimensional coordinate (or â€œvectorâ€) so that when you plot these word vectors in space, synonyms cluster. Hereâ€™s a visual:

![Word2vec illustration](/images/word2vec.png "Word2vec illustration")

*Words plotted in 3-dimensional space. Embeddings can have hundreds or thousands of dimensionsâ€“too many for humans to visualize.*

With Word2Vec, similar words cluster together in spaceâ€“so the vector/point representing â€œkingâ€ and â€œqueenâ€ and â€œprinceâ€ will all cluster nearby. Same thing with synonyms (â€œwalked,â€ â€œstrolled,â€ â€œjoggedâ€).

For other data types, itâ€™s the same thing. A song embedding would plot similar-sounding songs nearby. An image embedding would plot similar-looking images nearby. A customer-embedding would plot customers with similar buying habits nearby.

You can probably already see how this is useful: embeddings allow us to find similar data points. I could build a function, for example, that takes as input a word (i.e. â€œkingâ€) and finds me its ten closest synonyms. This is called a nearest neighbor search. Not terribly interesting to do with single words, but imagine instead if we embedded whole movie plots. Then we could build a function that, given the synopsis of one movie, gives us ten similar movies. Or, given one news article, recommends semantically similar articles.

Additionally, embeddings allow us to compute numerical similarity scores between embedded data points, i.e. â€œHow similar is this news article to that one?â€ One way to do this is to compute the distance between two embedded points in space and say that the closer they are, the more similar they are. This measure is also known as Euclidean distance. (You could also use dot product, cosine distance, and other trigonometric measures.)

Similarity scores are useful for applications like duplicate detection and facial recognition. To implement facial recognition, for example, you might embed pictures of peopleâ€™s faces, then determine that if two pictures have a high enough similarity score, theyâ€™re of the same person. Or, if you were to embed all the pictures on your cell phone camera and found photos that were very nearby in embedding space, you could conclude those points were likely near-duplicate photos.

Similarity scores can also be used for typo correction. In Word2Vec, common misspellingsâ€“â€hello,â€ â€œhelo,â€ â€œhelllo,â€ â€œhEeeeelOâ€--tend to have high similarity scores because theyâ€™re all used in the same contexts.

The graphs above also illustrate an additional and very neat property of Word2Vec, which is that different axes capture grammatical meaning, like gender, verb tense, and so on. This means that by adding and subtracting word vectors, we can solve analogies, like â€œman is to woman as king is to \_\_\_\_.â€ Itâ€™s quite a neat feature of word vectors, though this trait doesnâ€™t always translate in a useful way to embeddings of more complex data types, like images and longer chunks of text. (More on that in a second.)