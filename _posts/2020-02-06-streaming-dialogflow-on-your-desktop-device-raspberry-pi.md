---
layout: post
title: "Streaming Dialogflow on your Desktop/Device/Raspberry Pi"
description: "Authentication"
date: 2020-02-06
feature_image: 
tags: []
---
    
    
{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/0.png" title="" caption="" %}

_Turn your local computer into a listening, talking Voice User Interface powered by Dialogflow._

If you‚Äôve worked with [Dialogflow](https://dialogflow.com/) (DF) before, you know it‚Äôs a quick way to build smart conversational interfaces. A Dialogflow [Agent](https://cloud.google.com/dialogflow/docs/agents-overview) can be the brains behind a Facebook Messenger or Slack chatbot, a Google Assistant or Alexa app, or even an automated customer service agent.

<!--more-->

Out of the box, Dialogflow comes with a handful of supported integrations that make deploying a DF app on Slack/Facebook/Alexa/etc a snap.

{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/1" title="Some bulit-in Dialogflow integrations." caption="Some bulit-in Dialogflow integrations." %}



But sometimes, rather than integrate your agent into an existing platform, you want to run it on your own device. Maybe you‚Äôre building a self-help kiosk and want to talk to Dialogflow directly from your localMac/PC/Raspberry Pi.

In this post, I‚Äôll show you how to do just that, by connecting to a Dialogflow Agent in Node.js, streaming audio in from your computer‚Äôs microphone, and streaming your DialogFlow agent‚Äôs responses out through your speaker. (If you don‚Äôt use Node, check out a Python code sample [here](https://gist.github.com/dalequark/aa0385de55b97fe1e2950b6e1ead063c).) All you‚Äôll need to get started is an existing DF agent. Meanwhile, if you want to skip directly to the code, you can find it [here](https://gist.github.com/dalequark/4648c110b02963a049da2bfa637493fb).

Let‚Äôs get started üëâ

Configuring Dialogflow to Speak
===============================

First, log in to the [Dialogflow console](https://dialogflow.cloud.google.com/#/login) and navigate to the project you‚Äôd like to stream with. In my case, I‚Äôm working with a project called `SimpleAlarm`.

Click the gear icon in the upper left corner of the screen.

{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/2" title="Here, under the General tab, note your project id (mine is `simplealarm-spoitm`). Save that value for later." caption="Here, under the General tab, note your project id (mine is `simplealarm-spoitm`). Save that value for later." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/3" title="Next, navigate to the ‚ÄúSpeech‚Äù tab, scroll down and click the slider next to ‚ÄúEnable Automatic Text to Speech.‚Äù Enabling this option causes Dialogflow to return audio data in its responses, which can be played directly through the speaker." caption="Next, navigate to the ‚ÄúSpeech‚Äù tab, scroll down and click the slider next to ‚ÄúEnable Automatic Text to Speech.‚Äù Enabling this option causes Dialogflow to return audio data in its responses, which can be played directly through the speaker." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/4" title="If you scroll down here, you‚Äôll also be able to see various options for changing your agent‚Äôs voice. Under ‚ÄúVoice,‚Äù which by default is set to ‚ÄúAutomatic,‚Äù you can select alternate male and female voices as well as [WaveNet](https://cloud.google.com/text-to-speech/docs/wavenet) voices, which sound more human-like than the standard voices." caption="If you scroll down here, you‚Äôll also be able to see various options for changing your agent‚Äôs voice. Under ‚ÄúVoice,‚Äù which by default is set to ‚ÄúAutomatic,‚Äù you can select alternate male and female voices as well as [WaveNet](https://cloud.google.com/text-to-speech/docs/wavenet) voices, which sound more human-like than the standard voices." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/5" title="Click save, and we‚Äôre good to go." caption="Click save, and we‚Äôre good to go." %}



Authentication
--------------

Since we want to interact with our Dialogflow Agent in Node.js from our local computer, we‚Äôll need to set up authentication. Across Google Cloud, authentication is handled through [service accounts](https://cloud.google.com/iam/docs/service-accounts), which we‚Äôll set up now.

First, navigate to the GCP [console](https://console.cloud.google.com/) (now‚Äôs a good time to mention you‚Äôll need to have a Google Cloud account and login). Navigate to the GCP project whose project id corresponds to the project id of your Dialogflow Agent (remember we took note of that in the last step?).

In the left hand bar, go to IAM & admin -> Service accounts.

{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/6" title="Click ‚Äú+ CREATE SERVICE ACCOUNT‚Äù at the top of the screen." caption="Click ‚Äú+ CREATE SERVICE ACCOUNT‚Äù at the top of the screen." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/7" title="Create a new service account with some name that describes where you expect it to be used (in this case, on our desktop/raspi/etc)." caption="Create a new service account with some name that describes where you expect it to be used (in this case, on our desktop/raspi/etc)." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/8" title="Click create. On the next page, you‚Äôll be able to give permissions to your account. For now, all we‚Äôll need is the ‚ÄúDialogflow API Client‚Äù permission." caption="Click create. On the next page, you‚Äôll be able to give permissions to your account. For now, all we‚Äôll need is the ‚ÄúDialogflow API Client‚Äù permission." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/9" title="Finally, on the last screen, you‚Äôll see a button that says ‚Äú+ CREATE KEY.‚Äù Click that button." caption="Finally, on the last screen, you‚Äôll see a button that says ‚Äú+ CREATE KEY.‚Äù Click that button." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/10" title="Next, select ‚ÄúJSON‚Äù for Key type and select Create." caption="Next, select ‚ÄúJSON‚Äù for Key type and select Create." %}



{% include image_caption.html imageurl="/images/2020-02-06-streaming-dialogflow-on-your-desktop-device-raspberry-pi/11" title="This should download a `json` credentials file to your computer. Anyone with this file will be able to access your Dialogflow Agent, so keep this a secret and don‚Äôt check it in to any GitHub repos!" caption="This should download a `json` credentials file to your computer. Anyone with this file will be able to access your Dialogflow Agent, so keep this a secret and don‚Äôt check it in to any GitHub repos!" %}



Meanwhile, create a new folder for this project (i.e. `dialogflow_streaming_agent`) and move the credentials `json` file to that folder.

Writing Node.js Dialogflow Streaming Code
=========================================

Dependencies
------------

To get started, you‚Äôll need to download some dependencies. For this project, I used the package [node-record-lpcm16](https://www.npmjs.com/package/node-record-lpcm16) to record audio from my microphone and the package [node-speaker](https://github.com/TooTallNate/node-speaker) to play audio responses from Dialogflow through my speaker. You can use other speaker and microphone libraries, but I found these consistently worked on both my MacBook and on the Pi.

To install `node-record-lpcm16`, you may need to install some dependencies, like `sox`, first. See the docs [here](https://github.com/gillesdemey/node-record-lpcm16), but on a Mac, you‚Äôll probably want to do something like:

```
brew install sox  
npm install node-record-lpcm16
```

On Linux:

```
sudo apt-get install sox libsox-fmt-all  
npm install node-record-lpcm16
```

To install `node-speaker`:

```
npm install speaker
```

Or on Linux:

```
sudo apt-get install libasound2-dev  
npm install speaker
```

On my Mac, I noticed I kept getting the error `Illegal Instruction: 4` when using this library. The solution was to install `speaker` with this flag:

```
npm install speaker --mpg123-backend=openal
```

Finally:

```
npm install pump dotenv dialogflow-v2
```

Understanding the Dialogflow Streaming Interface
------------------------------------------------

**Dialogflow Streaming Configuration**

If you poke around the Dialogflow documentation, you‚Äôll see there is a sample code page called [Detect Intent with Audio Stream Input](https://cloud.google.com/dialogflow/docs/detect-intent-stream#detect-intent-stream-nodejs). This snippet shows you how to stream audio to and from _files_. But in this post, rather than stream from files, we want to collect audio directly from the microphone and play it directly out of the speaker. I‚Äôll walk you through how to do this step by step, but the completed code sample is [here](https://gist.github.com/dalequark/4648c110b02963a049da2bfa637493fb).

To get started with the Dialogflow streaming interface, import the library and create a new client:

```
const dialogflow = require('dialogflow');const sessionClient = new dialogflow.SessionsClient();
```

In this streaming setup, we‚Äôll collect audio from the microphone and continually send it to the `sessionClient`. But before we send audio, the first data packet we‚Äôll need to send to Dialogflow is a configuration object. I‚Äôve packaged this step up in a function I‚Äôve called `makeInitialStreamRequestArgs`:

```
function makeInitialStreamRequestArgs(projectId, sessionId) {  
    // Initial request for Dialogflow setup  
          
    const sessionPath = sessionClient.sessionPath(projectId, sessionId);  
          
    return {              
        session: sessionPath,              
        queryInput: {                  
            audioConfig: {                      
                audioEncoding: "LINEAR16",  
                sampleRateHertz: 16000,  
                languageCode: "en-US",  
            },                  
            singleUtterance: true,              
         },              
         outputAudioConfig: {                  
             audioEncoding: \`OUTPUT\_AUDIO\_ENCODING\_LINEAR\_16\`,  
             sampleRateHertz: 16000,              
         },          
    };      
}
```

Let‚Äôs talk about these configuration parameters.

First, `makeInitialStreamRequestArgs` takes two parameters, `projectId` and `sessionId`. `projectId` is your GCP project ID, which we collected from the Dialogflow console up above (in my case, it was `simplealarm-spoitm`). `sessionId`, on the other hand, is an id that you, the caller, create for each user _session_. You‚Äôll send this sessionId along with every call you make to Dialogflow to indicate you‚Äôre continuing an ongoing conversation.

These two parameters, `projectId` and `sessionId`, are used together to create a `sessionPath` variable which is sent along with the initial configuration.

You‚Äôll notice in the code above a `json` field called `queryInput`, which tells Dialogflow what type of input audio data to expect. We set`audioEncoding: "LINEAR16"` to indicate we‚Äôre sending audio from our microphone in ‚Äú16-bit linear pulse-code modulation (PCM) encoding‚Äù. This field can also be `"MP3"`, `"FLAC"`, or a handful of other encoding types listed [here](https://cloud.google.com/speech-to-text/docs/encoding). For raw data from a microphone, you‚Äôll usually want to use `"LINEAR16"`. `sampleRateHertz: 16000` indicates we‚Äôll send Dialogflow audio that‚Äôs sampled at 1600 Hertz. You can determine the sampling rate of your microphone on Linux using the command:

```
arecord --list-devices
```

However, I found that using an alternate sampling rate here doesn‚Äôt break anything ü§û.

In `queryInput`, we‚Äôll set the field `singleUtterance: true`. From the [docs](https://cloud.google.com/dialogflow/docs/detect-intent-stream):

*   If `false` (default), speech recognition does not cease until the client closes the stream.
*   If `true`, Dialogflow will detect a single spoken utterance in input audio. When Dialogflow detects the audio's voice has stopped or paused, it ceases speech recognition and sends a `StreamingDetectIntentResponse` with a recognition result of `END_OF_SINGLE_UTTERANCE` to your client. Any audio sent to Dialogflow on the stream after receipt of `END_OF_SINGLE_UTTERANCE` is ignored by Dialogflow.

Setting this field to `true` conveniently means that Dialogflow will automatically detect when the user has stopped speaking, allowing it to handle the logic of when to stop listening and start speaking its response.

Finally, `outputAudioConfig` tells Dialogflow what type of audio we‚Äôd like to receive, which we will then play through the speaker.

**Connecting Dialogflow to the Microphone**

To begin collecting audio and detecting intents, let‚Äôs create a new function, `getAudio(sessionId)`, which will handle creating a Dialogflow Stream, creating a microphone Stream, setting up configuration, and detecting intents. `getAudio` will return a [Javascript Promise](https://developers.google.com/web/fundamentals/primers/promises) which resolves when it‚Äôs a received an audio response from the Dialogflow (more details on that below).

First, create a new Dialogflow intent detection stream, like so:

```
function getAudio(sessionId, projectId) { const detectStream = this.sessionClient.streamingDetectIntent()            .on('error', console.error);
```

Next, we‚Äôll send the Dialogflow configuration packet using the function we wrote in the last step:

```
detectStream.write(makeInitialStreamRequestArgs(projectId, sessionId));
```

Now we‚Äôll begin collecting audio from the microphone and streaming it to the `detectStream` object.

To create a microphone stream, we‚Äôll create a new `record` object using the library `node-record-lpcm16` we installed before:

```
const recording = record.record({  
    sampleRateHertz: 16000,                  
    threshold: 0,                  
    verbose: false,                  
    recordProgram: 'arecord', // Try also "arecord" or "sox"  
    silence: '10.0',              
});              
const recordingStream = recording.stream().on('error', console.error);
```

You should set `recordProgram` to whatever audio recording software is installed on your computer. On my MacBook, I use `sox`, and on Linux/Raspberry Pi, `arecord`.

To connect the recording stream (`recordingStream`) to the Dialogflow stream (`detectStream`), I used a small Javascript library called [pump](https://www.npmjs.com/package/pump). This lets us do a small transform on the data coming out of the `recordingStream` so it‚Äôs in the format `detectStream` expects.

```
const pumpStream = pump(recordingStream,              
    // Format the audio stream into the request format.  
    new Transform({objectMode: true,  
    transform: (obj, \_, next) => {  
        next(null, { inputAudio: obj });  
    },}),              
    detectStream);
```

**Streaming Audio and Detecting Intents**

Now that we‚Äôre streaming audio data to the cloud, Dialogflow can listen and ‚Äî when it detects that the user is done speaking ‚Äî try to match the user‚Äôs [Intent](https://cloud.google.com/dialogflow/docs/intents-overview) and return a response.

Let‚Äôs take a look:

Streaming microphone data to Dialogflow and listening for Dialogflow‚Äôs responses

There‚Äôs a lot going on [here](http://baby-name-gen.firebaseapp.com), so let‚Äôs break it down.

`detectStream.on('data'` is a handler for listening to `data` events emitted by Dialogflow. Dialogflow emits `data` events to convey [lost of different types of information](https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.StreamingDetectIntentResponse):

```
{ responseId: '',  
  recognitionResult: null,  
  queryResult:  
   { fulfillmentMessages: \[\],  
     outputContexts: \[\],  
     queryText: '',  
     speechRecognitionConfidence: 0,  
     action: '',  
     parameters: null,  
     allRequiredParamsPresent: false,  
     fulfillmentText: '',  
     webhookSource: '',  
     webhookPayload: null,  
     intent: null,  
     intentDetectionConfidence: 0,  
     diagnosticInfo: null,  
     languageCode: 'en-US',  
     sentimentAnalysisResult: null },  
  webhookStatus: null,  
  outputAudio: <Buffer >,  
  outputAudioConfig: null }
```

For example, as the user speaks, Dialogflow will fire a `data` event with the parameter `data.recognitionResult` set, which contains a real-time transcript of what it thinks the user is saying. For example, if I say, into the microphone, ‚ÄúWhat‚Äôs the weather today?‚Äù Dialogflow might emit several `data` events with `recognitionResult.transcript` containing, respectively:

```
‚ÄúWhat‚Äôs‚Äù  
‚ÄúWhat‚Äôs the‚Äù  
‚ÄúWhat‚Äôs the heather‚Äù // NOTE: heather changed later to "weather"  
‚ÄúWhat‚Äôs the weather today?‚Äù
```

Note that sometimes (as in the example above), the transcription may change as the user speaks more words and provides more context.

When the user stops speaking, Dialogflow will emit a packet with the field `data.recognitionResult.isFinal` set to `true`. At this point, you‚Äôll **need** **close the microphone:**

```
if (data.recognitionResult.isFinal) {             
   console.log("Result Is Final");                      
   recording.stop();                      
}
```

When the user stops speaking, Dialogflow will begin matching what the user said to an Intent. It will then emit a data event with the field`data.queryResult` filled. `data.queryResult.fulfillmentText` will contain a text version of Dialogflow‚Äôs reply (i.e. ‚ÄúThe weather today is cloudy‚Äù). `queryResult` also contains [other useful fields](https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#queryresult), like `queryResult.intent.displayName` (the name of the matched intent) and `queryResult.parameters` for matched parameters.

Finally, the last `data` event Dialogflow emits will contain an audio response, `data.outputAudio`. At this point, in the code above, we close the pump (`pumpStream.end`) and resolve the Promise, return the audio and the `queryResult`:

```
resolve({‚Äúaudio‚Äù : data.outputAudio, ‚ÄúqueryResult‚Äù : queryResult});
```

By the way, if it‚Äôs unclear how this all fits together, check out the [complete code sample](https://gist.github.com/dalequark/4648c110b02963a049da2bfa637493fb) (so far we‚Äôve been talking about the `getAudio` function).

Congratulations! You‚Äôve set up the hardest part of streaming with Dialogflow.

**Playing Dialogflow‚Äôs Audio Responses through the Speaker**

After the user speaks a request to our agent and we receive its audio response, we‚Äôll need to play that response through our computer‚Äôs speaker using the `node-speaker` library. We‚Äôll do that in a function called `playAudio`:

```
playAudio(audioBuffer) {          
    return new Promise(resolve => {              
        // Setup the speaker for playing audio              
        const speaker = new Speaker({                  
            channels: 1,                  
            bitDepth: 16,                  
            sampleRate: 16000,              
        });                          
        speaker.on("close", () => {                  
            resolve();              
        });                  
        // Setup the audio stream, feed the audio buffer in     
        const audioStream = new PassThrough();  
        audioStream.pipe(speaker);  
        audioStream.end(audioBuffer);          
    })      
}
```

Because playing audio through the speaker is an asynchronous operation, `playAudio` returns a Promise that resolves when `audioBuffer` is done playing. Here, in the configuration for `Speaker`, `sampleRate` should be the same sample rate as the value we passed in `outputAudioConfig` when we configured our Dialogflow stream (in this case, 16000).

Now you have a function that plays audio through the speaker!

**Putting it All Together**

Now that we‚Äôve written code for getting and playing audio, let‚Äôs weave them together in a ‚Äúlisten-play-listen-play-listen‚Ä¶‚Äù loop. In other words, we‚Äôll design a new function that listens to the user, waits for them to stop speaking, speaks a responds, and begins listening again, just like a real (polite) human being.

Write a new function, `stream`, like this:

```
async function stream() {  
   console.log('Listening, press Ctrl+C to stop.');     
   // Create a new id for this session      
   const sessionId = uuidv1();          
         
   while (true) {          
       const res = await getAudio(sessionId, YOUR\_PROJECT\_ID);       
       if (res\["queryResult"\]) {  
           console.log("Got query result ", res\["queryResult"\]);        
       }          
       if (res\["audio"\]) {  
           await stream.playAudio(res\["audio"\]);          
       }  
}}
```

This asynchronous function `stream` creates a new session by generating a random session id. Then it creates a loop that collects data from the microphone, listens for an audio response from Dialogflow, and plays it back through the speaker. It does this in a `while(true)` loop, i.e. forever.

Now, if you make a call to `stream()` at the end of your Javascript file, you‚Äôre good to go! Or you can of course just download and run [my code](https://gist.github.com/dalequark/4648c110b02963a049da2bfa637493fb). (Apologies that it doesn‚Äôt _exactly_ line up with these inline samples, but they‚Äôre pretty close).

You‚Äôre Done!
============

Turn up the volume and start chatting. You‚Äôve just built your first speaking, listening, digital companion that will never tire of listening to you. Tell your friends _you‚Äôre welcome_.
